\documentclass[a4paper,10pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{bbold}
\usepackage{epstopdf}
\usepackage[section]{placeins}
\usepackage{subcaption}


\title{Learning theory for matrix inverse problems with gaussian priors}
\author{Jonas Adler  \\ {\small KTH, Elekta} \and Olivier Verdier \\ {\small KTH}}
\date{}

\begin{document}
	\maketitle
	
	Suppose $A : \mathbb{R}^n \to \mathbb{R}^m$ is a forward operator and we have data of the form
	\begin{align*}
		x \in&\ \mathcal{N}(0, \Sigma) \\
		y \in&\ \mathcal{N}(Ax, \Gamma)
	\end{align*}
	
	\paragraph{Theorem 1:}
	The maximum a posteriori (MAP) estimate of $x$ given $y$
	\[
	    A^{-1}_{MAP}(y) = \arg\max_{x'} P(x' | y)
	\]
	is
	\[
		A^{-1}_{MAP}(y) = (A^T \Gamma^{-1} A + \Sigma^{-1})^{-1} A^T \Gamma^{-1} y
	\]
	
	\paragraph{Proof:} We have
	\[
		P(x' | y) \propto \underbrace{P(y | x')}_{\mathcal{N}(Ax', I)} \hspace{2mm} \underbrace{P(x')}_{\mathcal{N}(0, I)} 
	\]
	Instead of maximizing the posterior, we minimize the log of the posterior
	\[
		\arg\max_{x'} P(x' | y) = \arg\min \log P(x' | y) = \arg\min \left( \log P(y | x') + \log P(x') \right)
	\]
	where we let
	\begin{align*}
		f(x') =&\ \log P(y | x') + \log P(x') \\
		=&\ \frac{1}{2} \| Ax - y \|_{\Gamma^{-1}}^2 + \frac{1}{2} \| x \|_{\Sigma^{-1}}^2
	\end{align*}
	we compute
	\[
		\nabla f = A^T \Gamma^{-1} (Ax - y) + \Sigma^{-1} x
	\]
	Setting this to zero gives the conditions for a maximum
	\[
		(A^T \Gamma^{-1} A + \Sigma^{-1}) x = A^T \Gamma^{-1} y
	\]
	which gives a maximum likelihood solution as
	\[
		x = (A^T \Gamma^{-1} A + \Sigma^{-1})^{-1} A^T \Gamma^{-1} y
	\]
	
	\paragraph{Theorem 2:} We have
	\[
		\arg \min_{B} \mathbb{E}_{(x, y)} \|B(y) - x\|_2^2 = \mathbb{E}_{x}\bigl(x \mid y \bigr)
	\]
	where the minimization is taken over all operators $\mathbb{R}^m \to \mathbb{R}^n$.\footnote{This is also given in Bishop, 1.5.5}
	
	\paragraph{Proof:} By the law of total probability
	\begin{align*}
		\mathbb{E}_{(x, y)} \|B(y) - x\|_2^2 = 
		\mathbb{E}_{y} \bigl(\mathbb{E}_{x}\bigl(\|B(y) - x\|_2^2 \mid y \bigr)\bigr)
	\end{align*}
	By the monotonicity of the expectation we have
	\[
		B(y) = \arg\min_{z} \mathbb{E}_{x}\bigl(\|z - x\|_2^2 \mid y \bigr)
	\]
	we find this by differentiating and setting to zero
	\[
	    0 =
	    \mathbb{E}_{x}\bigl(2 (z - x) \mid y \bigr) =
	    2\mathbb{E}_{x}\bigl(z \mid y \bigr) - 2\mathbb{E}_{x}\bigl(x \mid y \bigr)
	\]
	which gives
	\[
		B(y) = \mathbb{E}_{x}\bigl(x \mid y \bigr)
	\]
	
\end{document}
