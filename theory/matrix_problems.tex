\documentclass[a4paper,10pt]{article}

\usepackage[biblatex, theoremdefs]{allergy}

\addbibresource{refs.bib}


\title{Learning theory for matrix inverse problems with gaussian priors}
\author{Jonas Adler  \\ {\small KTH, Elekta} \and Olivier Verdier \\ {\small KTH}}
\date{}

\begin{document}
	\maketitle
	
\section{Maximum a posteriori estimate}

	Suppose $A : \mathbb{R}^n \to \mathbb{R}^m$ is a linear forward operator (a matrix) and we have data of the form
	\begin{align}
		x \in&\ \mathcal{N}(0, \Sigma) \nonumber \\
		y \in&\ \mathcal{N}(Ax, \Gamma) \label{eq:inv_prob}
	\end{align}	
	

  \begin{theorem}[{\cite[\S\,4.4]{Mu2012}}]
	The maximum a posteriori (MAP) estimate of $x$ given $y$
	\[
	    A^{-1}_{MAP}(y) = \arg\max_{x'} P(x' | y)
	\]
	is
	\[
		A^{-1}_{MAP}(y) = (A^T \Gamma^{-1} A + \Sigma^{-1})^{-1} A^T \Gamma^{-1} y
	\]
\end{theorem}

	\begin{proof} We have
	\[
		P(x' | y) \propto \underbrace{P(y | x')}_{\mathcal{N}(Ax', I)} \hspace{2mm} \underbrace{P(x')}_{\mathcal{N}(0, I)} 
	\]
	Instead of maximizing the posterior, we minimize the log of the posterior
	\[
		\arg\max_{x'} P(x' | y) = \arg\min \log P(x' | y) = \arg\min \left( \log P(y | x') + \log P(x') \right)
	\]
	where we let
	\begin{align*}
		f(x') =&\ \log P(y | x') + \log P(x') \\
		=&\ \frac{1}{2} \| Ax - y \|_{\Gamma^{-1}}^2 + \frac{1}{2} \| x \|_{\Sigma^{-1}}^2
	\end{align*}
	we compute
	\[
		\nabla f = A^T \Gamma^{-1} (Ax - y) + \Sigma^{-1} x
	\]
	Setting this to zero gives the conditions for a maximum
	\[
		(A^T \Gamma^{-1} A + \Sigma^{-1}) x = A^T \Gamma^{-1} y
	\]
	which gives a maximum likelihood solution as
	\[
		x = (A^T \Gamma^{-1} A + \Sigma^{-1})^{-1} A^T \Gamma^{-1} y
	\]
\end{proof}

	\paragraph{Connection to Tikhonov regularization:}
	
	By identification, this is equivalent to solving the Tikhonov regularized problem:
	
	\[
		\min_x \| \Gamma^{-1/2} \bigl(Ax - y\bigr) \| + \|\Sigma^{-1/2} x\|
	\]
	
\section{Best function estimate}
\begin{theorem}[{\cite[\S\,1.5.5]{Bi2006}}]
  We have
	\[
		\arg \min_{B} \mathbb{E}_{(x, y)} \|B(y) - x\|_2^2 = \mathbb{E}_{x}\bigl(x \mid y \bigr)
	\]
	where the minimization is taken over all functions $B \colon \mathbb{R}^m \to \mathbb{R}^n$.
\end{theorem} 

	\begin{proof} By the law of total probability
	\begin{align*}
		\mathbb{E}_{(x, y)} \|B(y) - x\|_2^2 = 
		\mathbb{E}_{y} \bigl(\mathbb{E}_{x}\bigl(\|B(y) - x\|_2^2 \mid y \bigr)\bigr)
	\end{align*}
	By the monotonicity of the expectation we have
	\[
		B(y) = \arg\min_{z} \mathbb{E}_{x}\bigl(\|z - x\|_2^2 \mid y \bigr)
	\]
	we find this by differentiating and setting to zero
	\[
	    0 =
	    \mathbb{E}_{x}\bigl(2 (z - x) \mid y \bigr) =
	    2\mathbb{E}_{x}\bigl(z \mid y \bigr) - 2\mathbb{E}_{x}\bigl(x \mid y \bigr)
	\]
	which gives
	\[
		B(y) = \mathbb{E}_{x}\bigl(x \mid y \bigr)
	\]
\end{proof} 

	\begin{theorem} A neural network, which we denote by $A^{-1}_{NN}$, will in the limit of infinite capacity and trained to solve the inverse problem equation \ref{eq:inv_prob} according to
	\[
		\arg \min_{A^{-1}_{NN}} \mathbb{E}_{(x, y)} \|A^{-1}_{NN}(y) - x\|_2^2
	\]
	is given by the MAP estimate:
	\[
		A^{-1}_{NN}(y) = (A^T \Gamma^{-1} A + \Sigma^{-1})^{-1} A^T \Gamma^{-1} y
	\]
\end{theorem}

	\begin{proof} By theorem 2, we have that the optimum solution of the training problem is given by the conditional expectation. Since neural networks are universal function approximators, this will be attainable in the limit of infinite capacity.
	
	Further, for Guassian distributions the mean and the mode coincide. Thus the conditional expectation, being a gaussian, is the MAP estimate.
\end{proof}

  

\printbibliography%
	
\end{document}
